---
title: "Case Study 4 - Group 4"
author:
- Annika Janson h11829506
- Jan Beck h11814291
- Franz Uchatzi h1451890
date: "13.12.2020"
output:
  pdf_document: default
  html_document:
    df.print: paged
header-includes:
- \usepackage{dcolumn}
- \renewcommand{\and}{\\}
---


```{r setup, include=FALSE}
library(car)
library(stargazer)
library(xtable)
library(extrafont)
library(tseries)
knitr::opts_chunk$set(warning = FALSE, echo = TRUE)
marketing <- read.csv("marketing.csv")
N <- nrow(marketing)
```

# 2 Model

## 2.1 Model estimation

### 2.1.1 and 2.1.2  

```{r, echo=FALSE}
marketing_lm1 <- lm(rating ~ rq + vo + wa + kr + education + gender + income + age + price, marketing)
marketing_lm2 <- lm(rating ~ 0 + rq + vo + wa + kr + ju + education + gender + income + age  + price, marketing)

```

```{r, results='asis', echo=FALSE}
stargazer(marketing_lm1, marketing_lm2, header=FALSE, align=TRUE, title="Model comparison")
```


*(See page 2 for model comparison and regression output.)*

The R^2 value of model 1 and 2 is __0.348__ and __0.828__ respectively. 

The estimates and standard errors for the non-brand explanatory variables of model 1 and 2 are identical.

The estimates for `rq`, `vo`, `wa`, `ju`/intercept, `education`, `income`, `age` and `price` are significant at the 5%-level.

### 2.2 

__Model 1__: the estimate for `kr` is __-0.287950__, which means that on average the rating is changing by __-0.2887950__ c.p. In other words, we shift the regression line down by 0.2887950.

__Model 2__: the estimate for `kr` is __20.560087__, this is the intercept for `kr`. On average, if the brand kr and all other variables were 0, the rating would be __20.560087__ c.p.

### 2.3

We can calculate the regression parameter associated with `kr` in Model 1 by subtracting the value of `ju` in Model 2 from the value of `kr`in Model 2. 

This is because `ju` was our reference group, so the intercept of Model 1 is equivalent to the intercept of `ju`, which is also shown in Model 2. Model 1 shows us the difference between choosing "kr" or any other group and Model 2 shows us each groups intercept. 

\newpage

### 2.4 

H0: $\beta_{wa} = 0$  
H1: $\beta_{wa} \neq 0$ 

In model 1, the p-value for $\beta_{wa}$ is __0.05641__. Therefore, for  $\alpha=0.05$, we can not reject the null hypothesis. We conclude, that there is no difference in the average rating between the brands `ju` and `wa` c.p.

*Bonus question: *

```{r, echo=FALSE}
linearHypothesis(marketing_lm2, c("wa=ju")) 
```

The F-test shows that the p-value again is __0.05641__, which is exactly the p-value we expected, as it was the one we could see in the results of `wa` in Model 1. 

### 2.5 
#### 2.5.1

To check whether the brand information is helpful to determine the rating of mineral water, we perform an F-test for Model 1 with the following H0 and H1. However, we need to exclude the variable `ju` as it acts as the baseline for the brand effect in Model 1.

H0: $\beta_{rq}=\beta_{vo}=\beta_{wa}=\beta_{kr}=0$  
H1: $H_0\text{ is not true.}$

```{r, echo=FALSE}

brand_test1 <- linearHypothesis(marketing_lm1, c("rq=0", "vo=0", "wa=0", "kr=0")) 


```

```{r, echo=FALSE}

brand_test1.p <- brand_test1$`Pr(>F)`[2]
brand_test1.F <- brand_test1$F[2]
brand_test1
```


After we run the test we find that the p-value is __`r brand_test1.p`__ and the F-statistic is __`r brand_test1.F`__.  
Therefore, we reject the null hypothesis that the coefficients for `rq`, `vo`, `wa` and `kr` are equal to 0 and should keep them in the model, c.p.


*Bonus question:* For the bonus question, we take same approach as for Model 1 with the difference that now, all brands of mineral water are included in the H0. In Model 2 the intercept $\beta_{0}$ is excluded. 

$H_0: \beta_{ju}=\beta_{rq}=\beta_{vo}=\beta_{wa}=\beta_{kr}=0$  
$H_1: H_0\ \text{is not true.}$

```{r, echo=FALSE}

brand_test2 <- linearHypothesis(marketing_lm2, c("rq=0", "ju=0", "vo=0", "wa=0", "kr=0"))
brand_test2
```

```{r, include=FALSE}

brand_test2.p <- brand_test2$`Pr(>F)`[2]
brand_test2.F <- brand_test2$F[2]
brand_test2
brand_test2.p
brand_test2.F

# not sure if the p-value is derived correctly, since it is exactly zero
```

We run an F-test test and find that the p-value is __`r brand_test2.p`__ and the F-statistic is __`r brand_test2.F`__. 
Therefore, we reject the null hypothesis that the coefficients for `ju`, `rq`, `vo`, `wa` and `kr` are equal to 0 and should keep them in the model, c.p.


#### 2.5.2

```{r, echo=FALSE}

marketing_lm3 <- lm(rating ~ education + gender + income + age + price, marketing)

r2_1 <- summary(marketing_lm1)$r.squared
adj.r2_1 <- summary(marketing_lm1)$adj.r.squared
r2_1.percent <- round((r2_1*100), digits=4)
r2_3 <- summary(marketing_lm3)$r.squared
adj.r2_3 <- summary(marketing_lm1)$adj.r.squared
r2_3.percent <- round((r2_3*100), digits=4)

aic_lm1 <- AIC(marketing_lm1)
aic_lm3 <- AIC(marketing_lm3)

bic_lm1 <- BIC(marketing_lm1)
bic_lm3 <- BIC(marketing_lm3)

K1 <- length(marketing_lm1$coefficients) -1
K2 <- length(marketing_lm3$coefficients) -1

x_2.5.2 <- c(K1, r2_1, adj.r2_1, aic_lm1, bic_lm1, K2, r2_3, adj.r2_3, aic_lm3, bic_lm3)
m_2.5.2 <- matrix(data=x_2.5.2, nrow=2, ncol=5, byrow=TRUE)
rownames(m_2.5.2) <- c("Model 1", "Model 3")
colnames(m_2.5.2) <- c("K", "R-squared", "Adj. R-squared", "AIC", "BIC")


dif_r2 <- r2_1-r2_3
dif_r2_percent <- round((dif_r2*100), digits=4)
dif_adj_r2 <- adj.r2_1 - adj.r2_3
dif_aic <- aic_lm3 - aic_lm1
dif_bic <- bic_lm3 - bic_lm1

```

For our Model 3, we remove all brand variables from Model 1.

```{r, results='asis', echo=FALSE}
stargazer(m_2.5.2, header=FALSE, align=TRUE, title="Model comparison")
```

The table above shows various model selection criteria for Model 1 and Model 3. We see that R-squared of Model 1 is __`r dif_r2`__ larger than for Model 3, suggesting that Model 1 explains __`r dif_r2_percent`__% more variation in rating can be explained with variation of the independent variables. However, Model 1 consists of 4 more explanatory variables than Model 3 and the R-squared increases for each additional explanatory variable added to the model. 

We therefore look at the adjusted R-squared next, which penalizes extra variables added to the model. Its values is the same for Model 1 and Model 3 respectively with __`r adj.r2_1`__ . This criterion suggests, that adding the brand variables does not increase goodness of fit.

Lastly, we compare the AIC and BIC values for each model and see that for Model 1 the AIC is __`r dif_aic`__ and the BIC is __`r dif_bic`__ units smaller than for model 3. The smaller AIC and BIC values of Model 1 indicate a better fit of the model in comparison to Model 3. By this criterion, Model 1 explains the changes in rating better than Model 3.


### 2.6

```{r, echo=FALSE}

resids <- residuals(marketing_lm1)

x <- model.matrix(marketing_lm1)

resids_man <- marketing$rating - x %*% marketing_lm1$coefficients
#all.equal( resids, c(resids_man), check.attributes = FALSE) 


## Histogramm residuals


hist(resids, breaks= 40, xlab = "Residuals", main= "")

## QQplot
qqnorm(resids)
qqline(resids, col= "green")

## Jarque bera test

JB <- tseries::jarque.bera.test(resids)
JB

```
H0: Residuals are normally distributed
H1: Residuals are not normally distributed

Histogram: Looking at the histogram, it does not look like a symmetric normal distribution around 0. The distribution seems slightly left-skewed and there are less values at the center than we would expect for a normal distribution. 

QQ-Plot: Till 1.5 it seems the residuals follow a normal distribution. But for values higher than 1.5, they seem to differ from normal distribution. 

Jarque-Bera-Test: The test confirms our observations from the histogram and the QQ-Plot. With X-squared = __36.525__ it is bigger than __6__, which is the limit. Additional the p-value is __1.172e-08__, so very small. At a 5%-level, the residuals are not normally distributed and we reject the H0. 

Summarizing our observations, our error term is not normally distributed, we have a problem with our model.

### 2.7
```{r, echo=FALSE}

# marketing_lm <- lm(rating ~ rq + vo + wa + kr + education + gender + income + age + price, marketing)

marketing_lm2.7_1 <- lm(rating ~ rq + vo + wa + kr + education + gender + income + age + price + kr:age , marketing)
# marketing_lm2.7_2 <- lm(rating ~ rq + vo + wa + kr + education + gender + income + age + price + kr:age + vo:income, marketing)
marketing_lm2.7_3 <- lm(rating ~ rq + vo + wa + kr + education + gender + income + age + price + kr:age + wa:price, marketing) #+ vo:income

# summary(marketing_lm)
# summary(marketing_lm2.7_1) # kr:age
# summary(marketing_lm2.7_2) # kr:age, vo:income
# summary(marketing_lm2.7_3) # kr:age, vo:income, wa:price



```

We add interactions between dummy variables and continuous explanatory variables in two steps. First, the interaction between `kr` and `age`. Second, interaction added is between the variables `wa` and `price`. The results between each step are shown in 2.8.

### 2.8

```{r, echo=FALSE}

# r2_1
r2_2.8_1 <- summary(marketing_lm2.7_1)$r.squared # kr:age R2
# r2_2.8_2 <- summary(marketing_lm2.7_2)$r.squared # kr:age, vo:income
r2_2.8_3 <- summary(marketing_lm2.7_3)$r.squared # kr:age, vo:income, wa:price

# adj.r2_1
adjr2_2.8_1 <- summary(marketing_lm2.7_1)$adj.r.squared
# adjr2_2.8_2 <- summary(marketing_lm2.7_2)$adj.r.squared
adjr3_2.8_3 <- summary(marketing_lm2.7_3)$adj.r.squared

# aic_lm1
aic_2.8_1 <- AIC(marketing_lm2.7_1)
# aic_2.8_2 <- AIC(marketing_lm2.7_2)
aic_2.8_3 <- AIC(marketing_lm2.7_3)

# bic_lm1
bic_2.8_1 <- BIC(marketing_lm2.7_1)
# bic_2.8_2 <- BIC(marketing_lm2.7_2)
bic_2.8_3 <- BIC(marketing_lm2.7_3)

Rs <- c(r2_1,
        r2_2.8_1,
        
        r2_2.8_3) #r2_2.8_2,

adjRs <- c(adj.r2_1,
        adjr2_2.8_1,
       
        adjr3_2.8_3) # adjr2_2.8_2,

aics <- c(aic_lm1,
        aic_2.8_1,
       
        aic_2.8_3) # aic_2.8_2,

bics <- c(bic_lm1,
        bic_2.8_1,
        
        bic_2.8_3) # bic_2.8_2,

x_2.8 <- c(Rs, adjRs, aics, bics)
m_2.8 <- matrix(data=x_2.8, nrow=3, ncol=4)
rownames(m_2.8) <- c("Model 1", "Step1 (kr:age)", "Step2 (wa:price)") # ("Model 1", "Step1 (kr:age)", "Step2 (vo:income)", "Step3 (wa:price)")
colnames(m_2.8) <- c("R-squared", "Adj. R-squared", "AIC", "BIC")


```

```{r, results='asis', echo=FALSE}
stargazer(m_2.8, align=TRUE, header=FALSE)
```

The table above shows the addition of each interaction between a pair of selected variables and the effect on R-squared, adjusted R-squared, AIC and BIC. As a reference we compare each change in the parameters with the respective parameters of Model 1. Furthermore, the next table shows the respective p-values as well as t-test results for each interaction term and step respectively.


```{r, echo=FALSE}

# p-values step 1 kr:age
pvalue1 <- as.vector(summary(marketing_lm2.7_1)$coefficients[,4]) # saves the p-values as vector
kr_age1 <- pvalue1[11]

# p-values step 2 vo:income
#pvalue2 <- as.vector(summary(marketing_lm2.7_2)$coefficients[,4]) # saves the p-values as vector
#kr_age2 <- pvalue2[11]
#vo_income2 <- pvalue2[12]

# p-values step 3 wa:price
pvalue3 <- as.vector(summary(marketing_lm2.7_3)$coefficients[,4]) # saves the p-values as vector
kr_age3 <- pvalue3[11]
# vo_income3 <- pvalue3[12]
wa_price3 <- pvalue3[12]

# t-test step 1 kr:age
ttest_2.8_kr1 <- summary(marketing_lm2.7_1)[["coefficients"]][, "t value"][11]

# t-test step 2 vo:income
#ttest_2.8_kr2 <- summary(marketing_lm2.7_2)[["coefficients"]][, "t value"][11]
#ttest_2.8_vo2 <- summary(marketing_lm2.7_2)[["coefficients"]][, "t value"][12]
#ttest_2.8_wa3 <- summary(marketing_lm2.7_2)[["coefficients"]][, "t value"][12]

# t-test step 3 wa:price
 ttest_2.8_kr3 <- summary(marketing_lm2.7_3)[["coefficients"]][, "t value"][11]
# ttest_2.8_vo3 <- summary(marketing_lm2.7_3)[["coefficients"]][, "t value"][12]
 ttest_2.8_wa3 <- summary(marketing_lm2.7_3)[["coefficients"]][, "t value"][12]


# p-values
pvalues1 <-  c(kr_age1)
pvalues2 <- c(kr_age3,
              wa_price3) 
#vo_income2
#pvalues3 <- c(kr_age3,
             # vo_income3)
              
# t-test values
ttests1 <- c(ttest_2.8_kr1)
ttests2 <- c(ttest_2.8_kr3,
             ttest_2.8_wa3)
              
#ttest_2.8_vo2
# ttests3 <- c(ttest_2.8_kr3,
           # ttest_2.8_vo3,
            # ttest_2.8_wa3)
            
x_2.8_tests <- c(pvalues1, pvalues2,  ttests1, ttests2) #, ttests3 pvalues3,
m_2.8_tests <- matrix(data=x_2.8_tests, nrow=2, ncol=4)
rownames(m_2.8_tests) <- c("Step1 (kr:age)", "Step2 (wa:price)") # "Step2 (vo:income)", "Step3 (wa:price)")
colnames(m_2.8_tests) <- c("p-values (1)", "p-values (2)", "t-test (1)", "t-test (2)") #("p-values (1)", "p-values (2)", "p-values (3)", "t-test (1)", "t-test (2)", "t-test (3)")

```

```{r, results='asis', echo=FALSE}
stargazer(m_2.8_tests, align=TRUE, header=FALSE)
```

\newpage

#### 2.8.1

```{r, echo=FALSE}

marketing_lm4 <- marketing_lm2.7_3
summary(marketing_lm4)

```


#### 2.8.2 

We interpret the interaction term between `kr` and `age`. The estimated coefficient in our model for the interaction term is __0.003347__. This means, that for every additional year a consumer would rate the mineral water of brand `kr` on average __0.003347__ higher. This could be the result of a marketing strategy that primarily targets older consumers.

It's important to note that this additional effect only applies to mineral waters of the brand `kr`. `age` and brand `kr` still have separate effects on the average rating, therefore we can not say "all else equal" in respect to the interaction.

### 2.9

From the output in 2.8.1 and Table 3 we can see that the two interaction terms we added are not significant at the 5%-level and did not increase goodness of fit by any measure. We therefore drop these terms and return to Model 1 and inspect and p-values of the included terms.

```{r, echo=FALSE}
summary(marketing_lm1)
```

Since `gender` has the highest p-value of the remaining variables, we drop it from the model next. 

```{r, echo=FALSE}

marketing_lm1_a <- lm(rating ~ rq + vo + wa + kr + education + income + age + price, marketing)

summary(marketing_lm1_a)

```
\newpage
Finally, we remove `education`:

```{r, echo=FALSE}

marketing_lm1_b <- lm(rating ~ rq + vo + wa + kr + income + age + price, marketing)

summary(marketing_lm1_b)

```

The final improved model has a BIC of `r BIC(marketing_lm1_b)` (recall that the BIC from Model 4, with which we started, was `r BIC(marketing_lm4)`). We have therefore improved goodness of fit even though we dropped 4 variables in the process. The variables that we kept are all significant at the 5%-level except for `wa` and `kr`.

\newpage

# 3 Theorie


### 3.1 

That is true. $R^2$ is always increasing with each additional variable, no matter how good the new variable is. In general SSR are always smaller than TSS, and $R^2$ is close to 1 the smaller SSR is.  If SSR = 0, then $R^2 = 1$. In this case we don't make any errors and were able to explain the variance of our model completely. In a model with a fixed number of observations N, $R^2$ will be always 1 if we add N-1 explanatory variables, no matter how useful they are. 

For example: 

```{r, echo=FALSE} 
chick <- read.csv("chicken.csv")
chick1 <- chick[c(1:5), c(1:5)]
model1 <- lm(log(consum) ~ log(income) + log(pchick) + log(pbeef) + log(ppork), data=chick1)
summary(model1)

# Only the first 5 observations, so we have N-1 explanatory variables.


```
We used the chicken data set to show that $R^2$ is increasing to 1, if we set the numbers of observations to explanatory variables + 1. We created a new data frame including all 4 explanatory variables (`income`, `pbeef`, `pchick`, `ppork`) and 5 observations. The result shows us the expected $R^2$ of 1.  


The adjusted R^2 in comparison, is taking in to account how good the new variable is. So the $ R^2adj $ is only increasing, if the change in $R^2$ is large. 

The formula: $R^2_{adj} = 1 - \frac{N-1}{N-K-1} * (1-R^2)$ So with increasing "K", the term 1 $\frac{N-1}{N-K-1}$ gets bigger and $R^2adj$ smaller, but with the term $(1-R^2)$ it is still increasing if the change is large.  

\newpage
### 3.2

We consider the model $Y=\beta_0+\beta_1X+\beta_2X^2+u$ 

First, we should test if we should include the quadratic term or not:

H0: $\beta_2=0$  
H1: $\beta_2\neq0$

We can use a t-test to that end. If $\beta_0\neq0$, non-linearity is given in our model and we should keep the quadratic term. 

Next, to check whether the sign changes at 1 for X, we use the following formula 

$$
\frac{\partial \mathbb{E}(Y \mid X=x)}{\partial x}=\beta_{1}+2 \beta_{2} x=0
$$
which yields that sign change  lies at 

$$
X_0=-\beta_1/(2\beta_2)
$$

Since we test for $X_0=1$, our null hypothesis is:

$$
\beta_1+2\beta_2=0\quad\text{or:}\quad\beta_2+\frac{\beta_1}{2}=0
$$

Only if $\beta_1$ and $\beta_2$ have different signs, the vertex can be positive, as in our case where $X_0=1$.


We can test this using an F-test.

\newpage

### 3.3

We simulate non-linear data where the sign change occurs at $X_0=1$ and plot the result:

```{r} 

set.seed(1)

# our parameters
N <- 200
beta0 <- 50
beta1 <- 3.5
beta2 <- -beta1/2 # sign change at 1
mu <- 0
sigma <- 3
minX = -4
maxX = 6

# our model 
x <- runif(N, min = minX, max = maxX)
u <- rnorm(N, sd = sigma, mean = mu)
y <- beta0 + beta1*x + beta2*x^2 + u

# plot data
plot( x, y, xlim = c(minX, maxX) )
abline(v=1, col="red")
```
\newpage

Next, we try to estimate a model with a quadratic term

```{r} 
qm <- lm(y ~ x + I(x^2)) 
summary(qm)

```

The regression output shows that the p-value of the quadratic term is very low (significant) and the adjusted R-squared is __0.943__ (the model fits the data well). 

For comparison, we estimate a model without a quadratic term:

```{r} 

lm <- lm(y ~ x) 
summary(lm)

```

The p-value of $\beta_1$ alone is now __0.758__ and the adjusted R-squared __-0.009225__. Therefore, keeping the quadratic term is a good idea.

Lastly, we run an F-test to see if the hypothesis $\beta_1+2\beta_2=0$ holds:

```{r} 

linearHypothesis(qm, c("x+2*I(x^2)=0"))

```

From the F-test we obtain an F-statistic of __0.0128__ and a p-value of __0.9101__. Therefore, we find little evidence in the data that we should reject the hypothesis that the sign chance occurs at $X_0=1$.
