---
title: "Case Study 3 - Group 4"
author:
- Annika Janson h11829506
- Jan Beck h11814291
- Franz Uchatzi h1451890
date: "29.11.2020"
output:
  pdf_document: default
  html_document:
    df.print: paged
  word_document: default
header-includes:
- \usepackage{dcolumn}
- \renewcommand{\and}{\\}
---


```{r setup, include=FALSE}
library(car)
library(stargazer)
library(xtable)
library(extrafont)
library(greekLetters)
library(xtable)

options(xtable.floating = FALSE)
options(xtable.timestamp = "")
options(xtable.comment = FALSE)

knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
wages <- read.csv("WD1.csv")
N <- nrow(wages)
```

# 2 Descriptive statistics

```{r, results='asis', echo=FALSE}
stargazer(wages, omit = c("X"), header=FALSE, type='latex', summary.stat=c('mean', 'sd', 'median', 'min', 'max'), title='Summary statistics', align=TRUE)
```
\newpage
### 2.1

The average wage is USD __`r round(mean(wages$wage), 2)`__ and the median wage is USD __`r median(wages$wage)`__.

```{r, echo=FALSE}
hist(wages$wage, main="Histogram of wage", xlab="Wage in USD")
legend("topright", legend=c("Mean","Median"), lwd=c(2,2), col=c("red","blue"))
abline(v = mean(wages$wage), col = "red", lwd = 2)
abline(v = median(wages$wage), col = "blue", lwd = 2)

```

In the histogram we see that the distribution is right-skewed with a few observations exceeding USD 3000. Most observed values are concentrated around an interval of USD Â±500 above and below the mean. Median and mean are fairly close to each other with the median being slightly higher due to the large outliers.

### 2.2

The proportion of workers working more than 40 hours a week is __`r length(which(wages$hours > 40)) / N * 100`%__.

### 2.3

The most common number of years of education among the workers is __`r row.names(cbind(sort(table(wages$educ),decreasing=TRUE)))[1]`__.

### 2.4

No. The most frequent sibling pattern is having __`r names(sort(table(wages$sibs), decreasing=TRUE))[1]`__ sibling.

```{r, echo=FALSE}
par(mfrow = c(1,2))
barplot(sort(table(wages$educ),decreasing=TRUE), 
  xlab="years of education",
  ylab="count"
)
barplot(sort(table(wages$sibs), decreasing=TRUE), 
  xlab="number of siblings",
  ylab="count"
)
```

\newpage
# 3 Data modelling

### 3.1

```{r, echo=FALSE}
wage = wages$wage
hours = wages$hours
educ = wages$educ
exper = wages$exper
tenure = wages$tenure
age = wages$age
iq = wages$IQ
sibs = wages$sibs
brthord = wages$brthord
meduc = wages$meduc
feduc = wages$feduc

#wage.lm = lm(wage ~ hours + educ + exper + tenure + age + iq + sibs + brthord + meduc + feduc, data=wd)
#summary(wage.lm)

# reg1 = lm(log(wage) ~ log(hours) + log(educ) + log(exper) + log(tenure) + log(age) + log(iq) + log(sibs) + log(brthord) + log(meduc) + log(feduc), data=wd)

lwage = lm(log(wage) ~ hours + educ + exper + tenure + age + iq + sibs + brthord + meduc + feduc, data=wages)


#summary(lwage)
reg1.fit = fitted(lwage)
#reg1.fit

lwage_B0 <- as.vector(coef(lwage)[1]) # intercept
lwage_B1 <- as.vector(coef(lwage)[2]) # beta1hat hours
lwage_B2 <- as.vector(coef(lwage)[3]) # beta2hat educ
lwage_B3 <- as.vector(coef(lwage)[4]) # beta3hat exper
lwage_B4 <- as.vector(coef(lwage)[5]) # beta4hat tenure
lwage_B5 <- as.vector(coef(lwage)[6]) # beta5hat age
lwage_B6 <- as.vector(coef(lwage)[7]) # beta6hat iq
lwage_B7 <- as.vector(coef(lwage)[8]) # beta7hat sibs
lwage_B8 <- as.vector(coef(lwage)[9]) # beta8hat brthord
lwage_B9 <- as.vector(coef(lwage)[10]) # beta9hat meduc
lwage_B10 <- as.vector(coef(lwage)[11]) # beta10hat feduc

r2 <- summary(lwage)$r.squared
r2.percent <- round((r2*100), digits=4)
```

```{r, results='asis', echo=FALSE}
stargazer(lwage, 
  header=FALSE, 
  type='latex', 
  title='Model summary', 
  align=TRUE, 
  report="vc*sp",
  single.row=TRUE,
  ci.custom = list(confint(lwage))
)
```

(Parenthesis show 95%-confidence intervals.)

The coefficient of determination is __`r r2`__. Therefore, __`r r2.percent`__ % of the variation of the dependent variable wage is explained by variation of the independent variables of the current model.

\newpage

### 3.2

```{r, echo=FALSE}
lwage_B2 <- as.vector(coef(lwage)[3]) # beta2hat educ
lwage_B2.percent <- round((lwage_B2 * 100), digits=3)

```

If we increase X (education) by one year we expect wage to increase approximately^[100 * $\hat{\beta}_{j}$%] by __`r lwage_B2.percent`__% (or by __`r 100 * ( exp(lwage_B2) - 1)`__%  exactly^[100 * ($e^{\hat{\beta}}_{j}$ - 1)%]), c.p.


### 3.3

```{r, echo=FALSE}
lwage_B8 <- as.vector(coef(lwage)[9]) # beta8hat brthord

pvalue <- as.vector(summary(lwage)$coefficients[,4]) # saves the p-values as vector
brthord.p <- pvalue[9] # shows the p-value of the variable brthord
lwage_B8.percent <- round((lwage_B8 * 100), digits=3)
```

For each additional older sibling we expect the wage to decrease by approximately^1^ __`r abs(lwage_B8.percent)`__ % (or by __`r abs(100 * ( exp(lwage_B8) - 1))`__%  exactly^2^), c.p. However, the p-value of the variable `brthord` is __`r brthord.p`__. This indicates that we can not reject the null hypothesis $\hat{\beta}_{8}$ = 0 when using a significance level of $\alpha=0.05$. 


### 3.4

```{r, echo=FALSE}
lwage_B2 <- as.vector(coef(lwage)[3]) # beta2hat educ
lwage_B9 <- as.vector(coef(lwage)[10]) # beta9hat meduc
lwage_B10 <- as.vector(coef(lwage)[11]) # beta10hat feduc
lwage_B2.percent <- round((lwage_B2 * 100), digits=3)
lwage_B9.percent <- round((lwage_B9 * 100), digits=3)
lwage_B10.percent <- round((lwage_B10 * 100), digits=3)

```

If we increase X (education) by three years we expect wage to increase approximately^1^ by __`r lwage_B2.percent * 3`__% (or by __`r 3 * 100 * ( exp(lwage_B2) - 1)`__%  exactly^2^), c.p.  
  
Analogously, if we increase X (years of education of mother) by three years we expect wage to increase approximately^1^ by __`r lwage_B9.percent * 3`__% (or by __`r 3 * 100 * ( exp(lwage_B9) - 1)`__%  exactly^2^), c.p.

And if we increase X (years of education of father) by three year we expect wage to increase approximately^1^ by __`r lwage_B10.percent * 3`__% (or by __`r 3 * 100 * ( exp(lwage_B10) - 1)`__%  exactly^2^), c.p.  
  
We can see that the largest average effect on wage with three additional years of education of the variables educ, meduc and feduc is achieved by the education of the workers themselves.

This is because the coefficient for `educ` (`r lwage_B2`) is larger than that of `meduc` (`r lwage_B9`) and `feduc` (`r lwage_B10`).

Note that the effects of `meduc` and `feduc` are not significant at the 5%-level.


### 3.5.1

```{r, echo=FALSE}

educ.p <- pvalue[3] # shows the p-value of the variable educ
educ.t <- summary(lwage)$coef[,3][3] # shows the test statistic of the variable educ

```

$H_0: \beta_{educ}=0$  
$H_1: \beta_{educ}\neq0$

The p-value is __`r format(educ.p, scientific=FALSE)`__ and the t-statistic is __`r format(educ.t, scientific=FALSE)`__. Therefore, we __reject__ the null hypothesis at the 5% significance level and assume that an additional year of education has  influence on wage, c.p.


### 3.5.2

```{r, echo=FALSE}
brthord.p <- pvalue[9] # pvalue of brthord from model lwages
brthord.t <- summary(lwage)$coef[,3][9] # shows the test statistic of the variable educ

```

$H_0: \beta_{brthord}=0$  
$H_1: \beta_{brthord}\neq0$

The p-value is __`r format(brthord.p, scientific=FALSE)`__ and the t-statistic is __`r format(brthord.t, scientific=FALSE)`__. Therefore, we do __not reject__ the null hypothesis at the 5% significance level and assume that the variable birth order has no influence on wages, c.p.

### 3.5.3

```{r, echo=FALSE}
lwage_test1 <- linearHypothesis(lwage, c("sibs=0", "brthord=0", "meduc=0", "feduc=0")) # h0: the variables number of siblings, birth order, years of education of mother/father have no influence on wages jointly
lwage_test1.p <- lwage_test1$`Pr(>F)`[2]
lwage_test1.F <- lwage_test1$F[2]
```

$H_0: \beta_{sibs}=\beta_{brthord}=\beta_{meduc}=\beta_{feduc}=0$  
$H_1: H_0\ \text{is not true.}$ 

As we can see in the summary in 3.1 the variables with a p-value < 0.05 are `sibs`, `brthord`, `meduc`, `feduc` and, therefore, have on average no significant influence on wages individually.  

We run a linear hypthesis test and find that the p-value is __`r lwage_test1.p`__ and the F-statistic is __`r lwage_test1.F`__. Therefore, we __reject__ the null hypothesis at the 5% significance level and assume that the coefficients for `sibs`, `brthord`, `meduc` and `feduc` can not be jointly excluded from the model c.p. as at least one of them is different from 0. However, we do not know which one.

  
```{r, results='asis', echo=FALSE}
stargazer(lwage_test1, 
  header=FALSE, 
  summary=FALSE,
  type='latex', 
  title='Linear hypothesis test: sibs=0 brthord=0 meduc=0 feduc=0', 
  align=TRUE
)
```

### 3.5.4

```{r, echo=FALSE}
lwage_test2 <- linearHypothesis(lwage, c("sibs=0", "brthord=0", "meduc=0")) # h0: the variables number ofund  siblings, birth order, years of education of mother have no influence on wages jointly
lwage_test2.p <- lwage_test2$`Pr(>F)`[2]
lwage_test2.F <- lwage_test2$F[2]
```

$H_0: \beta_{sibs}=\beta_{brthord}=\beta_{meduc}=0$   
$H_1: H_0\ \text{is not true.}$ 

We run a linear hypthesis test and find that the p-value is __`r lwage_test2.p`__ and the F-statistic is __`r lwage_test2.F`__. 
We find little evidence in the data that we should reject the null hypothesis that the coefficients for `sibs`, `brthord`, `meduc` and `feduc` are equal to 0 and therefore can be jointly excluded from the model, c.p. 

  
```{r, results='asis', echo=FALSE}
stargazer(lwage_test2, 
  header=FALSE, 
  summary=FALSE,
  type='latex', 
  title='Linear hypothesis test: sibs=0 brthord=0 meduc=0', 
  align=TRUE
)
```

### 3.5.5

```{r, echo=FALSE}
lwage_test3 <- linearHypothesis(lwage, c("meduc=feduc")) # h0: The variables years of education mother and years of education father have the same effect on wages, c.p. 
lwage_test3.p <- lwage_test3$`Pr(>F)`[2]
lwage_test3.F <- lwage_test3$F[2]

```

$H_0: \beta_{meduc}-\beta_{feduc}=0$ or: $H_0: \beta_{meduc}=\beta_{feduc}$  
$H_1: \beta_{meduc}-\beta_{feduc}\neq0$ 

We run a linear hypthesis test and find that the p-value is __`r lwage_test3.p`__ and the F-statistic is __`r lwage_test3.F`__. 
We find little evidence in the data that we should reject the null hypothesis that the coefficients for `meduc` and `feduc` are the same, c.p. 
  
```{r, results='asis', echo=FALSE}
stargazer(lwage_test3, 
  header=FALSE, 
  summary=FALSE,
  type='latex', 
  title='Linear hypothesis test: meduc-feduc=0', 
  align=TRUE
)
```

\newpage

### 3.5.6

Franz:

Annika: Yes. It seems that an additional year of education has the largest effect on wage and other variables are not as important. So with continuing my studies at WU I expect an increase in my future wage. Beside of my interest in learning, a increase in wage is something motivating me to continue my studies. 

Jan: I would not overinterpret the results for my own career. The r^2 is relatively low, the data is 40 years old and we don't know where the sample was taken from. In addition, there is some evidence that suggests that returns on education is not linear due to the sheepskin effect ^[https://www.nas.org/blogs/article/the_sheepskin_effect]


\newpage

## 4 Simulation Study

### 4.1 

```{r}
set.seed(1)

# our parameters according to spec
N1 <- 10
N2 <- 100
N3 <- 1000
beta0 <- -1
beta1 <- 0.2
mu <- 0
sigma <- sqrt(4)
minX = -3
maxX = 3

# model 1
x1 <- x <- runif(N1, min = minX, max = maxX)
u1 <- rnorm(N1, sd = sigma, mean = mu)
y1 <- beta0 + beta1*x1 + u1
lm1 <- lm(y1 ~ x) # using x instead of x1 to show as one row in stargazer output 

# model 2
x2 <- x <- runif(N2, min = minX, max = maxX)
u2 <- rnorm(N2, sd = sigma, mean = mu)
y2 <- beta0 + beta1*x2 + u2
lm2 <- lm(y2 ~ x) # using x instead of x2 to show as one row in stargazer output

# model 3
x3 <- x <- runif(N3, min = minX, max = maxX)
u3 <- rnorm(N3, sd = sigma, mean = mu)
y3 <- beta0 + beta1*x3 + u3
lm3 <- lm(y3 ~ x) # using x instead of x3 to show as one row in stargazer output

```


```{r, results='asis', echo=FALSE}

invisible(stargazer(lm1, lm2, lm3, header=FALSE, align=TRUE, digits=7, title="Model comparison"))

```




```{r, echo=FALSE}

# This allows three plots to be displayed next to one another
par(mfrow = c(1,3), mar=c(5, 3, 5, 0) + 0.1)

# plot model 1

plot(
  x1, y1, 
  main = bquote(N ~ "=" ~ .(N1)), 
  sub = bquote(hat(beta)[0] ~ "=" ~ .(round(lm1$coef[1], 2)) ~"    " ~ hat(beta)[1] ~ "=" ~ .(round(lm1$coef[2], 2))),
  xlim = c(minX,maxX), ylim = c(-8, 8), 
  col = "gray", 
  xlab = "", ylab = "")
abline(beta0, beta1, lty=2)
abline(lm1)

legend("topright", legend=c("estimated slope","true slope"), lty=c(1,2))

# plot model 2

plot(x2, y2, main = bquote(N ~ "=" ~ .(N2)), sub = bquote(hat(beta)[0] ~ "=" ~ .(round(lm2$coef[1], 2)) ~
                             "    " ~ hat(beta)[1] ~ "=" ~ .(round(lm2$coef[2], 2))),
     xlim = c(minX,maxX), ylim = c(-8, 8), col = "gray", xlab = "", ylab = "")

abline(beta0, beta1, lty=2)
abline(lm2)

legend("topright", legend=c("estimated slope","true slope"), lty=c(1,2))



# plot model 3

plot(x3, y3, main = bquote(N ~ "=" ~ .(N3)), sub = bquote(hat(beta)[0] ~ "=" ~ .(round(lm3$coef[1], 2)) ~
                             "    " ~ hat(beta)[1] ~ "=" ~ .(round(lm3$coef[2], 2))),
     xlim = c(minX,maxX), ylim = c(-8, 8), col = "gray", xlab = "", ylab = "")
abline(beta0, beta1, lty=2)
abline(lm3)

legend("topright", legend=c("estimated slope","true slope"), lty=c(1,2))


```

\newpage
### 4.2 

Calculated 95%-confidence intervals and standard errors for beta0 and beta1 and different Ns:

```{r, echo=FALSE}

alpha <- 0.05
K <- 1

# model 1
lm1beta0 <- as.vector(summary(lm1)$coef[,1][1])
lm1beta0se <- as.vector(summary(lm1)$coef[,2][1])
lm1beta1 <- as.vector(summary(lm1)$coef[,1][2])
lm1beta1se <- as.vector(summary(lm1)$coef[,2][2])
qt1 <- qt(1-alpha/2, df=N1-K-1)
ci1beta0 <- c(lm1beta0 - qt1 * lm1beta0se, lm1beta0 + qt1 * lm1beta0se)
ci1beta1 <- c(lm1beta1 - qt1 * lm1beta1se, lm1beta1 + qt1 * lm1beta1se)

# all.equal(as.vector(confint(lm1, 1)), ci1beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm1, 2)), ci1beta1) # TRUE means we calculated correctly


# model 2
lm2beta0 <- as.vector(summary(lm2)$coef[,1][1])
lm2beta0se <- as.vector(summary(lm2)$coef[,2][1])
lm2beta1 <- as.vector(summary(lm2)$coef[,1][2])
lm2beta1se <- as.vector(summary(lm2)$coef[,2][2])
qt2 <- qt(1-alpha/2, df=N2-K-1)
ci2beta0 <- c(lm2beta0 - qt2 * lm2beta0se, lm2beta0 + qt2 * lm2beta0se)
ci2beta1 <- c(lm2beta1 - qt2 * lm2beta1se, lm2beta1 + qt2 * lm2beta1se)

# all.equal(as.vector(confint(lm2, 1)), ci2beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm2, 2)), ci2beta1) # TRUE means we calculated correctly


# model 3
lm3beta0 <- as.vector(summary(lm3)$coef[,1][1])
lm3beta0se <- as.vector(summary(lm3)$coef[,2][1])
lm3beta1 <- as.vector(summary(lm3)$coef[,1][2])
lm3beta1se <- as.vector(summary(lm3)$coef[,2][2])
qt3 <- qt(1-alpha/2, df=N3-K-1)
ci3beta0 <- c(lm3beta0 - qt3 * lm3beta0se, lm3beta0 + qt3 * lm3beta0se)
ci3beta1 <- c(lm3beta1 - qt3 * lm3beta1se, lm3beta1 + qt3 * lm3beta1se)

# all.equal(as.vector(confint(lm3, 1)), ci3beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm3, 2)), ci3beta1) # TRUE means we calculated correctly


```

```{r, results='asis', echo=FALSE}
# build a table for the listing the CIs
ciSummary <- data.frame(
  N = c(N1,N2,N3), 
  beta0 = c(
    paste("(", round(ci1beta0[1], 4),", ", round(ci1beta0[2], 4), ")", sep=""), 
    paste("(", round(ci2beta0[1], 4),", ", round(ci2beta0[2], 4), ")", sep=""), 
    paste("(", round(ci3beta0[1], 4),", ", round(ci3beta0[2], 4), ")", sep="")), 
  beta1 = c(
    paste("(", round(ci1beta1[1], 4),", ", round(ci1beta1[2], 4), ")", sep=""), 
    paste("(", round(ci2beta1[1], 4),", ", round(ci2beta1[2], 4), ")", sep=""), 
    paste("(", round(ci3beta1[1], 4),", ", round(ci3beta1[2], 4), ")", sep="")),
  se0 = c(
    lm1beta0se,
    lm2beta0se,
    lm3beta0se),
  se1 = c(
    lm1beta1se,
    lm2beta1se,
    lm3beta1se)
)
stargazer(ciSummary, summary=FALSE, rownames=FALSE, align=TRUE, header=FALSE)
```

We observe that the standard errors decrease for greater Ns and therefore the confidence intervals get smaller. This means that we become more certain of our estimation. For example, only with N=1000 does the confidence interval for $\beta_1$ not include 0 and at the 5%-confidence interval, we could therefore reject the null hypothesis $\beta_1=0$.

This shows that the OLS estimator is __consistent__ i.e. for ever larger Ns the estimator converges "in probability" to the true value of $\beta$. 

The reason for this can be seen in the formula for the standard error:

$$
\operatorname{se}\left(\hat{\beta}_{j}|X\right)=\frac{\hat{\sigma}}{\sqrt{n} \operatorname{sd}\left(x_{j}\right) \sqrt{1-R_{j}^{2}}}
$$

All else equal, when $\sqrt{n}$ in the denominator approaches $\infty$, the overall result approaches 0.

\newpage
### 4.3

For this we run 3 more experiments, this time with values of 2, 5 and 8 for $\sigma^2$:

```{r}

N <- (N3)
var <- c(2,5,8)
sig <- sqrt(var)
x <- runif(N, -3, 3)
u1 <- rnorm (N, 0, sig[1])
u2 <- rnorm (N, 0, sig[2])
u3 <- rnorm (N, 0, sig[3])

# model 4
x1 <- x <- runif(N3, min =minX, max =maxX)
u1 <- rnorm(N3, sd =sig[1], mean = mu)
y1 <- beta0 + beta1*x1 + u1
lm4 <- lm(y1 ~ x1) 


# model 5
x2 <- x <- runif(N3, min = minX, max = maxX)
u2 <- rnorm(N3, sd = sig[2], mean = mu)
y2 <- beta0 + beta1*x2 + u2
lm5 <- lm(y2 ~ x2) 

# model 6
x3 <- x <- runif(N3, min = minX, max = maxX)
u3 <- rnorm(N3, sd = sig[3], mean = mu)
y3 <- beta0 + beta1*x3 + u3
lm6 <- lm(y3 ~ x3) 


```

```{r, echo=FALSE}

alpha <- 0.1
K <- 1

# model 4
lm4beta0 <- as.vector(summary(lm4)$coef[,1][1])
lm4beta0se <- as.vector(summary(lm4)$coef[,2][1])
lm4beta1 <- as.vector(summary(lm4)$coef[,1][2])
lm4beta1se <- as.vector(summary(lm4)$coef[,2][2])
qt1 <- qt(1-alpha/2, df=N1-K-1)
ci1beta0 <- c(lm4beta0 - qt1 * lm4beta0se, lm4beta0 + qt1 * lm4beta0se)
ci1beta0_sig1_alp1<- abs(ci1beta0[1]-ci1beta0[2])
ci1beta1 <- c(lm4beta1 - qt1 * lm4beta1se, lm4beta1 + qt1 * lm4beta1se)
ci1beta1_sig1_alp1<- abs(ci1beta1[1]-ci1beta1[2])

# all.equal(as.vector(confint(lm4, 1)), ci1beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm4, 2)), ci1beta1) # TRUE means we calculated correctly


# model 5
lm5beta0 <- as.vector(summary(lm5)$coef[,1][1])
lm5beta0se <- as.vector(summary(lm5)$coef[,2][1])
lm5beta1 <- as.vector(summary(lm5)$coef[,1][2])
lm5beta1se <- as.vector(summary(lm5)$coef[,2][2])
qt2 <- qt(1-alpha/2, df=N2-K-1)
ci2beta0 <- c(lm5beta0 - qt2 * lm5beta0se, lm5beta0 + qt2 * lm5beta0se)
ci2beta0_sig2_alp1<- abs(ci2beta0[1]-ci2beta0[2])
ci2beta1 <- c(lm5beta1 - qt2 * lm5beta1se, lm5beta1 + qt2 * lm5beta1se)
ci2beta1_sig2_alp1<- abs(ci2beta1[1]-ci2beta1[2])

# all.equal(as.vector(confint(lm5, 1)), ci2beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm5, 2)), ci2beta1) # TRUE means we calculated correctly


# model 6
lm6beta0 <- as.vector(summary(lm6)$coef[,1][1])
lm6beta0se <- as.vector(summary(lm6)$coef[,2][1])
lm6beta1 <- as.vector(summary(lm6)$coef[,1][2])
lm6beta1se <- as.vector(summary(lm6)$coef[,2][2])
qt3 <- qt(1-alpha/2, df=N3-K-1)
ci3beta0 <- c(lm6beta0 - qt3 * lm6beta0se, lm6beta0 + qt3 * lm6beta0se)
ci3beta0_sig3_alp1<- abs(ci3beta0[1]-ci3beta0[2])
ci3beta1 <- c(lm6beta1 - qt3 * lm6beta1se, lm6beta1 + qt3 * lm6beta1se)
ci3beta1_sig3_alp1<- abs(ci3beta1[1]-ci3beta1[2])

# all.equal(as.vector(confint(lm6, 1)), ci3beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm6, 2)), ci3beta1) # TRUE means we calculated correctly

alpha <- 0.05
K <- 1

# model 4
lm4beta0 <- as.vector(summary(lm4)$coef[,1][1])
lm4beta0se <- as.vector(summary(lm4)$coef[,2][1])
lm4beta1 <- as.vector(summary(lm4)$coef[,1][2])
lm4beta1se <- as.vector(summary(lm4)$coef[,2][2])
qt1 <- qt(1-alpha/2, df=N1-K-1)
ci1beta0 <- c(lm4beta0 - qt1 * lm4beta0se, lm4beta0 + qt1 * lm4beta0se)

ci1beta0_sig1_alp2<- abs(ci1beta0[1]-ci1beta0[2])
ci1beta1 <- c(lm4beta1 - qt1 * lm4beta1se, lm4beta1 + qt1 * lm4beta1se)

ci1beta1_sig1_alp2<- abs(ci1beta0[1]-ci1beta0[2])

# all.equal(as.vector(confint(lm4, 1)), ci1beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm4, 2)), ci1beta1) # TRUE means we calculated correctly


# model 5
lm5beta0 <- as.vector(summary(lm5)$coef[,1][1])
lm5beta0se <- as.vector(summary(lm5)$coef[,2][1])
lm5beta1 <- as.vector(summary(lm5)$coef[,1][2])
lm5beta1se <- as.vector(summary(lm5)$coef[,2][2])
qt2 <- qt(1-alpha/2, df=N2-K-1)
ci2beta0 <- c(lm5beta0 - qt2 * lm5beta0se, lm5beta0 + qt2 * lm5beta0se)

ci2beta0_sig2_alp2<- abs(ci2beta0[1]-ci2beta0[2])
ci2beta1 <- c(lm5beta1 - qt2 * lm5beta1se, lm5beta1 + qt2 * lm5beta1se)
ci2beta1_sig2_alp2<- abs(ci2beta1[1]-ci2beta1[2])

# all.equal(as.vector(confint(lm5, 1)), ci2beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm5, 2)), ci2beta1) # TRUE means we calculated correctly


# model 6
lm6beta0 <- as.vector(summary(lm6)$coef[,1][1])
lm6beta0se <- as.vector(summary(lm6)$coef[,2][1])
lm6beta1 <- as.vector(summary(lm6)$coef[,1][2])
lm6beta1se <- as.vector(summary(lm6)$coef[,2][2])
qt3 <- qt(1-alpha/2, df=N3-K-1)
ci3beta0 <- c(lm6beta0 - qt3 * lm6beta0se, lm6beta0 + qt3 * lm6beta0se)
ci3beta0_sig3_alp2<- abs(ci3beta0[1]-ci3beta0[2])
ci3beta1 <- c(lm6beta1 - qt3 * lm6beta1se, lm6beta1 + qt3 * lm6beta1se)
ci3beta1_sig3_alp2<- abs(ci3beta1[1]-ci3beta1[2])

# all.equal(as.vector(confint(lm6, 1)), ci3beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm6, 2)), ci3beta1) # TRUE means we calculated correctly


alpha <- 0.01
K <- 1

# model 4
lm4beta0 <- as.vector(summary(lm4)$coef[,1][1])
lm4beta0se <- as.vector(summary(lm4)$coef[,2][1])
lm4beta1 <- as.vector(summary(lm4)$coef[,1][2])
lm4beta1se <- as.vector(summary(lm4)$coef[,2][2])
qt1 <- qt(1-alpha/2, df=N1-K-1)
ci1beta0 <- c(lm4beta0 - qt1 * lm4beta0se, lm4beta0 + qt1 * lm4beta0se)
ci1beta0_sig1_alp3<- abs(ci1beta0[1]-ci1beta0[2])
ci1beta1 <- c(lm4beta1 - qt1 * lm4beta1se, lm4beta1 + qt1 * lm4beta1se)
ci1beta1_sig1_alp3<- abs(ci1beta1[1]-ci1beta1[2])

# all.equal(as.vector(confint(lm4, 1)), ci1beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm4, 2)), ci1beta1) # TRUE means we calculated correctly


# model 5
lm5beta0 <- as.vector(summary(lm5)$coef[,1][1])
lm5beta0se <- as.vector(summary(lm5)$coef[,2][1])
lm5beta1 <- as.vector(summary(lm5)$coef[,1][2])
lm5beta1se <- as.vector(summary(lm5)$coef[,2][2])
qt2 <- qt(1-alpha/2, df=N2-K-1)
ci2beta0 <- c(lm5beta0 - qt2 * lm5beta0se, lm5beta0 + qt2 * lm5beta0se)
ci2beta0_sig2_alp3<- abs(ci2beta0[1]-ci2beta0[2])
ci2beta1 <- c(lm5beta1 - qt2 * lm5beta1se, lm5beta1 + qt2 * lm5beta1se)
ci2beta1_sig2_alp3<- abs(ci2beta1[1]-ci2beta1[2])

# all.equal(as.vector(confint(lm5, 1)), ci2beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm5, 2)), ci2beta1) # TRUE means we calculated correctly


# model 6
lm6beta0 <- as.vector(summary(lm6)$coef[,1][1])
lm6beta0se <- as.vector(summary(lm6)$coef[,2][1])
lm6beta1 <- as.vector(summary(lm6)$coef[,1][2])
lm6beta1se <- as.vector(summary(lm6)$coef[,2][2])
qt3 <- qt(1-alpha/2, df=N3-K-1)
ci3beta0 <- c(lm6beta0 - qt3 * lm6beta0se, lm6beta0 + qt3 * lm6beta0se)
ci3beta0_sig3_alp3<- abs(ci3beta0[1]-ci3beta0[2])
ci3beta1 <- c(lm6beta1 - qt3 * lm6beta1se, lm6beta1 + qt3 * lm6beta1se)
ci3beta1_sig3_alp3<- abs(ci3beta1[1]-ci3beta1[2])

# all.equal(as.vector(confint(lm6, 1)), ci3beta0) # TRUE means we calculated correctly
# all.equal(as.vector(confint(lm6, 2)), ci3beta1) # TRUE means we calculated correctly

```


```{r, echo=FALSE}
b01 <- c(ci1beta0_sig1_alp1, ci1beta0_sig1_alp2, ci1beta0_sig1_alp3)
b02 <- c(ci2beta0_sig2_alp1, ci2beta0_sig2_alp2, ci2beta0_sig2_alp3)
b03 <- c(ci3beta0_sig3_alp1, ci3beta0_sig3_alp2, ci3beta0_sig3_alp3)
MatrixB0 <- cbind(b01, b02, b03)

alpha <- c(0.1, 0.05, 0.01)

colnames (MatrixB0)<- sig
rownames (MatrixB0) <- alpha

c01 <- c(ci1beta1_sig1_alp1, ci1beta1_sig1_alp2, ci1beta1_sig1_alp3)
c02 <- c(ci2beta1_sig2_alp1, ci2beta1_sig2_alp2, ci2beta1_sig2_alp3)
c03 <- c(ci3beta1_sig3_alp1, ci3beta1_sig3_alp2, ci3beta1_sig3_alp3)

MatrixC0 <-  cbind(c01, c02, c03)

colnames (MatrixC0)<- var
rownames (MatrixC0) <- alpha

```

The following table shows the absolute size of the confidence intervals for different significance levels of the 3 models. The first column shows the value for $\alpha$ and the table head shows the value for $\sigma^2$:

```{r, results='asis', echo=FALSE}

xtable(MatrixC0, digits=6)

```

If we increase the level of confidence e.g to 99%, then the confidence interval gets wider and if we decrease the level of confidence to e.g 90%, the confidence interval gets narrower. Therefore a 99% confidence interval is less precise/ uncertain. 
If we increase the variance of the error term the confidence interval gets wider and less precise. If we decrease the error variance it is easier to predict the model, so the confidence interval gets narrower. 
If we combine a high confidence level and increase the error term variance at the same time we get an even wider confidence interval.

