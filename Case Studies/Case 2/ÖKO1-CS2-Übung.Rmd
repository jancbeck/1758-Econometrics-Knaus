---
title: "Case Study 2 - Group 4"
author:
- Annika Janson h11829506
- Jan Beck h11814291
- Franz Uchatzi
date: "8.11.2020"
output:
  pdf_document: default
  html_document:
    df.print: paged
header-includes: 
 - \usepackage{dcolumn}
 - \renewcommand{\and}{\\}
---


```{r setup, include=FALSE}
library(stargazer)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
chick <- read.csv("chicken.csv")
```

## 3 Data Analysis

### 3.1

```{r, echo=FALSE}

price <- chick$pchick
demand <- chick$consum
N <- nrow(chick)

plot(x = price,
     y = demand,
     xlab="Price", #Beschriftung x-Achse
     ylab="Demand", #Beschriftung y-Achase
     main="Scatterplot") #Beschriftung Grafik
     #abline(lm(y~x), col="red"), #regression line (y~x)
     #lines(lowess(x,y), col="blue")) # lowess line (x,y)
      

```
- positive, but economics theory would suggest a negative correlation

\newpage
### 3.2 

```{r, echo=FALSE}
reg1 <- lm(log(consum) ~ log(income) + log(pchick) + log(pbeef) + log(ppork), data=chick) # attention logs!
#summary(reg1)

m1_B0 <- as.vector(coef(reg1)[1]) # intercept
m1_B1 <- as.vector(coef(reg1)[2]) # beta1hat income
m1_B2 <- as.vector(coef(reg1)[3]) # beta2hat chicken
m1_B3 <- as.vector(coef(reg1)[4]) # beta3hat beef
m1_B4 <- as.vector(coef(reg1)[5]) # beta4hat pork

```

```{r, results='asis', echo=FALSE}
invisible(stargazer(reg1, header=FALSE, type='latex', title="Regression results", align=TRUE))

```


- result suggest that the higher the price for chicken the lower demand for it (negative correlation)
- compatible with economic theory
- extra question: 1) so that we can interpret it as elasticities (% changes) 2) to make the correlation "more linear"


### 3.3

- A 1% increase of income leads to a __`r m1_B1`__% change in demand.


### 3.4

- A 2% increase of income leads to a __`r 2*m1_B1`__% change in demand.


### 3.5

```{r, echo=FALSE}

log_demand <- predict(reg1, data.frame(income=2200, pchick=50, ppork=170, pbeef=312)) 


```

- The log demand is __`r log_demand`__ 

- The demand is __`r exp(log_demand)`__ 

### 3.6

```{r, echo=FALSE}

fitted <- fitted(reg1)
r <- cov(fitted, log(demand)) / (sd(fitted) * sd(log(demand)))
                                 
SSR <- sum((log(demand) - fitted )^2) # or: sum(resid(reg1)^2)
#SSR == sum(resid(reg1)^2) # not sure why its not the same?
TSS <- sum((log(demand) - mean(log(demand)))^2)
#TSS == var(log(demand)) * ( length(demand) - 1) # equals alternate formula: N * var(Y) 

r2 <- 1 - SSR/TSS
#r2 == summary(reg1)$r.squared  # compare our result with regression output
 
```

- The correlation R is __`r r`__
- The coefficient of determination is __`r r2`__


### 3.7

#### i)
```{r, echo=FALSE}

biased_residVar <- SSR / ( N - 1 ) # sum of squared residuals / N-1 
#biased_residVar == var(resid(reg1)) # not sure why its not the same?

```

- The biased estimator of the variance of the residuals is __`r format(round(biased_residVar, 11), nsmall = 11)`__

#### ii)

```{r, echo=FALSE}

K <- length( reg1$coefficients ) - 1 # number of predictor variables X1, . . . , XK.
unbiased_residVar <- SSR / ( N - K - 1 ) # SSR/(N-K-1)

```

- The unbiased estimator of the error variance $\hat\sigma^2$ is __`r format(round(unbiased_residVar, 11), nsmall = 11)`__

#### iii)

```{r, results='asis', echo=FALSE}

invisible(stargazer(vcov(reg1), header=FALSE, align=TRUE, title="Covariance Matrix"))

```

### 3.8
```{r, echo=FALSE}

model2 <- lm(log(consum) ~ log(income), data=chick)
#summary(model2)
m2_B1 <- as.vector(coef(model2)[2])


model3 <- lm(log(consum) ~ log(pchick), data=chick)
#summary(model3)
m3_B1 <- as.vector(coef(model3)[2])


```

```{r, results='asis', echo=FALSE}

invisible(stargazer(model2, model3, header=FALSE, align=TRUE, title="Regression results"))

```

- $\hat{\beta}_{1}$ is __`r m1_B1`__ and $\hat{\alpha}_{1}$ is __`r m2_B1`__.

- $\hat{\beta}_{2}$ is __`r m1_B2`__ and $\hat{\alpha}_{2}$ is __`r m3_B1`__.



\newpage
## 4 Theory

### 4.1 

#### 4.1.1

By definition study+ sleep+ work +leisure= 168. Therfore, if we change study we need to change at least one of the other categories so that they still sum up to 168.

#### 4.1.2

This model violates MLR.3 (No Perfect Collinearity). There is a perfect linearity between study, sleep, work and leisure, since study+ sleep +work +leisure= 168, so these variables exhibit perfect multicollinearity. To not violate Assumption MLR.3 it is possible to drop one of the independent variables, so that: 
$GPA= \beta_1+ \beta_1study+ \beta_2sleep + \beta_3 work +u.$ Now if we increase e.g study by 1, $\beta_1$ is the change in GPA when we increase study by 1 hour but hold sleep, work constant. 



### 4.2

To answer this question we look at the components of the equation that define the standard error of the OLS estimators of the coefficient  $\tilde{\beta}_{1}$ and $\hat{\beta}_{j}$ for the simple and multiple regression model respectively:

$$
\begin{aligned}
\operatorname{se}(\tilde{\beta}_{1})&=\frac{\hat{\sigma}}{\sqrt{n} \operatorname{sd}(x_{j}) } \\ \\
\operatorname{se}(\hat{\beta}_{j})&=\frac{\hat{\sigma}}{\sqrt{n} \operatorname{sd}(x_{j}) \sqrt{1-R_{j}^{2}}}
\end{aligned}
$$
This allows us to see that given the same sample and error variance $\sigma^{2}$, the variance of the OLS estimators of the coefficient in the multiple regression model can only ever be equal or larger than that of the simple regression model because of $1-R_{j}^{2}$. 
"$x_{1}$ is almost uncorrelated" implies a value for $R_{j}^{2}$ that is close to zero. 

Next, we estimate the error variance $\hat{\sigma}^{2}$ by dividing the sum of squared residuals (SSR) by the degrees of freedom (df):

$$
\begin{aligned}
\hat{\sigma}^{2} = \frac{\sum\limits_{i=1}^n \hat{u}_{i}^{2}}{(n - k - 1)} = \frac{SSR}{df}
\end{aligned}
$$

In the case of the simple regression k=1, therefore in the multiple regression model the denominator has to be smaller and is increasing the error variance (and in turn, standard error). 

However, we know that the OLS residuals are defined by

$$
\hat{u}_{i}=y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1} x_{i 1}-\hat{\beta}_{2} x_{i 2}-\ldots-\hat{\beta}_{k} x_{i k}
$$

If "$x_{2}$ and $x_{3}$ have large partial effects on y", then the residuals are likely to be smaller unless the effects of $x_{2}$ and $x_{3}$ are smaller than those of $x_{1}$ alone in the simple regression model. Whether this is enough to outweigh the larger effects the additional parameters we can not say, because we don't know how large $x_{1}$ will be after $x_{2}$ and $x_{3}$ have been included in the model.
